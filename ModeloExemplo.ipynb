{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projeto.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNyRcYmaUIY96caP1NoN+LJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VjorgeSN/Tensorflow/blob/main/ModeloExemplo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7JWHYPEHoI3"
      },
      "source": [
        "#Instalação do tf model maker para criar o modelo\n",
        "!pip install tflite-model-maker"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUlq1EwdINWC"
      },
      "source": [
        "#Importação dos pacotes necessários\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "from tflite_model_maker import configs\n",
        "from tflite_model_maker import ExportFormat\n",
        "from tflite_model_maker import image_classifier\n",
        "from tflite_model_maker import ImageClassifierDataLoader\n",
        "from tflite_model_maker import model_spec\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFk9wx9OIoV8",
        "outputId": "ee0d0efd-4e7b-438d-c8a4-e8594da9e864",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Vamos pegar algumas imagens para brincar com este exemplo simples de ponta a ponta. \n",
        "#Centenas de imagens é um bom começo para Model Maker, enquanto mais dados podem alcançar melhor precisão.\n",
        "\n",
        "image_path = tf.keras.utils.get_file(\n",
        "    'flower_photos',\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "    untar=True\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
            "228818944/228813984 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhYMQN8GKehm",
        "outputId": "7ff61910-bcd6-4239-b011-8b9632865e4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Etapa 1. Carregar dados de entrada específicos para um aplicativo ML no dispositivo. \n",
        "#Dividindo-o em dados de treinamento e dados de teste.\n",
        "\n",
        "data = ImageClassifierDataLoader.from_folder(image_path)\n",
        "train_data, test_data = data.split(0.9)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Load image with size: 3670, num_label: 5, labels: daisy, dandelion, roses, sunflowers, tulips.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7SlrSU_MIKp",
        "outputId": "a9ec2f13-9ce5-4aa4-8157-8ae69b1fefae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Etapa 2. Personalize o modelo do TensorFlow.\n",
        "\n",
        "model = image_classifier.create(train_data)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Retraining the models...\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "hub_keras_layer_v1v2 (HubKer (None, 1280)              3413024   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1280)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 6405      \n",
            "=================================================================\n",
            "Total params: 3,419,429\n",
            "Trainable params: 6,405\n",
            "Non-trainable params: 3,413,024\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "103/103 [==============================] - 158s 2s/step - loss: 1.0882 - accuracy: 0.6485\n",
            "Epoch 2/5\n",
            "103/103 [==============================] - 157s 2s/step - loss: 0.6707 - accuracy: 0.8909\n",
            "Epoch 3/5\n",
            "103/103 [==============================] - 157s 2s/step - loss: 0.6239 - accuracy: 0.9165\n",
            "Epoch 4/5\n",
            "103/103 [==============================] - 156s 2s/step - loss: 0.5975 - accuracy: 0.9267\n",
            "Epoch 5/5\n",
            "103/103 [==============================] - 157s 2s/step - loss: 0.5871 - accuracy: 0.9324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYp2JLhoMbfB",
        "outputId": "c24b21a7-7ab8-4709-c794-25781ff257ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Etapa 3. Avalie o modelo.\n",
        "\n",
        "loss, accuracy = model.evaluate(test_data)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 21s 1s/step - loss: 0.6066 - accuracy: 0.9155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18F-pUTrRPV1",
        "outputId": "b93a81c2-08c6-4743-8485-ef92d5509d07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Etapa 4. Exportar para o modelo TensorFlow Lite.\n",
        "#Aqui, exportamos o modelo TensorFlow Lite com metadados que fornecem um padrão para as descrições do modelo. O arquivo de etiqueta está embutido em metadados.\n",
        "#Você pode baixá-lo na barra lateral esquerda da mesma forma que a parte de upload para seu próprio uso.\n",
        "\n",
        "model.export(export_dir='.')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:109: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:109: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:109: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:109: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpgzrh03k9/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpgzrh03k9/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Label file is inside the TFLite model with metadata.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Label file is inside the TFLite model with metadata.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving labels in /tmp/tmp7fpj39f8/labels.txt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving labels in /tmp/tmp7fpj39f8/labels.txt.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RErVb3mxThN5"
      },
      "source": [
        "#Processo detalhado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIDDZGQwTvpV"
      },
      "source": [
        "Etapa 1: carregar dados de entrada específicos para um aplicativo de ML no dispositivo\n",
        "O conjunto de dados de flores contém 3670 imagens pertencentes a 5 classes. Baixe a versão do arquivo do conjunto de dados e descompacte-o.\n",
        "\n",
        "O conjunto de dados tem a seguinte estrutura de diretório:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygEncJxtl-nQ"
      },
      "source": [
        "\n",
        "\n",
        "<pre>\n",
        "<b>flower_photos</b>\n",
        "|__ <b>daisy</b>\n",
        "    |______ 100080576_f52e8ee070_n.jpg\n",
        "    |______ 14167534527_781ceb1b7a_n.jpg\n",
        "    |______ ...\n",
        "|__ <b>dandelion</b>\n",
        "    |______ 10043234166_e6dd915111_n.jpg\n",
        "    |______ 1426682852_e62169221f_m.jpg\n",
        "    |______ ...\n",
        "|__ <b>roses</b>\n",
        "    |______ 102501987_3cdb8e5394_n.jpg\n",
        "    |______ 14982802401_a3dfb22afb.jpg\n",
        "    |______ ...\n",
        "|__ <b>sunflowers</b>\n",
        "    |______ 12471791574_bb1be83df4.jpg\n",
        "    |______ 15122112402_cafa41934f.jpg\n",
        "    |______ ...\n",
        "|__ <b>tulips</b>\n",
        "    |______ 13976522214_ccec508fe7.jpg\n",
        "    |______ 14487943607_651e8062a1_m.jpg\n",
        "    |______ ...\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBWcCkgvUBhJ"
      },
      "source": [
        "#image_path = tf.keras.utils.get_file(\n",
        "      #'flower_photos',\n",
        "      #'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "      #untar=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilTC2fqbUSyN"
      },
      "source": [
        "Use a classe ImageClassifierDataLoader para carregar dados.\n",
        "\n",
        "Quanto ao método from_folder (), ele pode carregar dados da pasta. Ele assume que os dados de imagem da mesma classe estão no mesmo subdiretório e o nome da subpasta é o nome da classe. Atualmente, imagens codificadas em JPEG e imagens codificadas em PNG são suportadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "029pmrbJUeDc"
      },
      "source": [
        "#data = ImageClassifierDataLoader.from_folder(image_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVJdypvpUplp"
      },
      "source": [
        "Divida em dados de treinamento (80%), dados de validação (10%, opcional) e dados de teste (10%)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lq4jV08U0oa"
      },
      "source": [
        "#train_data, rest_data = data.split(0.8)\n",
        "#validation_data, test_data = rest_data.split(0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ2TgICsVDHi"
      },
      "source": [
        "Mostra 25 exemplos de imagem com rótulos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_UZSPTCVA40"
      },
      "source": [
        "#plt.figure(figsize=(10,10))\n",
        "#for i, (image, label) in enumerate(data.dataset.take(25)):\n",
        "  #plt.subplot(5,5,i+1)\n",
        "  #plt.xticks([])\n",
        "  #plt.yticks([])\n",
        "  #plt.grid(False)\n",
        "  #plt.imshow(image.numpy(), cmap=plt.cm.gray)\n",
        "  #plt.xlabel(data.index_to_label[label.numpy()])\n",
        "#plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r30Dtbu-VcRO"
      },
      "source": [
        "Etapa 2: personalizar o modelo do TensorFlow\n",
        "Crie um modelo de classificador de imagem personalizado com base nos dados carregados. O modelo padrão é EfficientNet-Lite0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHIPfJRhVfaQ"
      },
      "source": [
        "#model = image_classifier.create(train_data, validation_data=validation_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVR6Nw5aVqK2"
      },
      "source": [
        "Vendo a estrutura detalhada do modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYF3uwC_Vsr5"
      },
      "source": [
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6h3hq-cVyDO"
      },
      "source": [
        "Etapa 3: Avalie o modelo personalizado\n",
        "Avalie o resultado do modelo, obtenha a perda e a precisão do modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBUF5oaWV2Sx"
      },
      "source": [
        "#loss, accuracy = model.evaluate(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejfVbTvsWBrg"
      },
      "source": [
        "Podemos traçar os resultados previstos em 100 imagens de teste. Rótulos previstos com cor vermelha são os resultados previstos incorretos, enquanto outros estão corretos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHLITCkXWC3w"
      },
      "source": [
        "# Uma função auxiliar que retorna 'vermelho' / 'preto' dependendo se suas duas entradas\n",
        "# parâmetro corresponde ou não.\n",
        "#def get_label_color(val1, val2):\n",
        "  #if val1 == val2:\n",
        "    #return 'black'\n",
        "  #else:\n",
        "    #return 'red'\n",
        "\n",
        "# Em seguida, plote 100 imagens de teste e seus rótulos previstos.\n",
        "# Se um resultado de previsão for diferente do rótulo fornecido, rótulo em \"teste\"\n",
        "# conjunto de dados, iremos destacá-lo na cor vermelha.\n",
        "#plt.figure(figsize=(20, 20))\n",
        "#predicts = model.predict_top_k(test_data)\n",
        "#for i, (image, label) in enumerate(test_data.dataset.take(100)):\n",
        "  #ax = plt.subplot(10, 10, i+1)\n",
        "  #plt.xticks([])\n",
        "  #plt.yticks([])\n",
        "  #plt.grid(False)\n",
        "  #plt.imshow(image.numpy(), cmap=plt.cm.gray)\n",
        "\n",
        "  #predict_label = predicts[i][0][0]\n",
        "  #color = get_label_color(predict_label,\n",
        "                          #test_data.index_to_label[label.numpy()])\n",
        "  #ax.xaxis.label.set_color(color)\n",
        "  #plt.xlabel('Predicted: %s' % predict_label)\n",
        "#plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TMLnzeeXR20"
      },
      "source": [
        "Se a precisão não atender aos requisitos do aplicativo, pode-se consultar o Uso avançado para explorar alternativas, como mudar para um modelo maior, ajustar os parâmetros de retreinamento etc.\n",
        "\n",
        "Link uso avançado: https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb?hl=pt_br#scrollTo=zNDBP2qA54aK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBvD9llsYQGW"
      },
      "source": [
        "Etapa 4: exportar para o modelo TensorFlow Lite\n",
        "Converta o modelo existente para o formato de modelo TensorFlow Lite com metadados. O nome de arquivo TFLite padrão é model.tflite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRqeD0WqYRqu"
      },
      "source": [
        "#model.export(export_dir='.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzewfm-HYeom"
      },
      "source": [
        "See example applications and guides of image classification for more details about how to integrate the TensorFlow Lite model into mobile apps.\n",
        "[example applications and guides of image classification](https://www.tensorflow.org/lite/models/image_classification/overview#example_applications_and_guides)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT5vOLTRY88R"
      },
      "source": [
        "Os formatos de exportação permitidos podem ser um ou uma lista dos seguintes:\n",
        "\n",
        "ExportFormat.TFLITE\n",
        "ExportFormat.LABEL\n",
        "ExportFormat.SAVED_MODEL\n",
        "Por padrão, ele apenas exporta o modelo TensorFlow Lite com metadados. Você também pode exportar arquivos diferentes seletivamente. Por exemplo, exportando apenas o arquivo de etiqueta da seguinte forma:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHoO89fVYteU"
      },
      "source": [
        "#model.export(export_dir='.', export_format=ExportFormat.LABEL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3EqvJ7vZKw1"
      },
      "source": [
        "Você também pode avaliar o modelo tflite com o método evaluate_tflite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O0sIDTbZRvH"
      },
      "source": [
        "#model.evaluate_tflite('model.tflite', test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHTaSWtWaLBs"
      },
      "source": [
        "## Uso Avançado\n",
        "\n",
        "A função `create` é a parte crítica desta biblioteca. Ele usa a aprendizagem por transferência com um modelo pré-treinado semelhante ao [tutorial] (https://www.tensorflow.org/tutorials/images/transfer_learning).\n",
        "\n",
        "A função `create` contém as seguintes etapas:\n",
        "\n",
        "1. Divida os dados em treinamento, validação e teste de dados de acordo com o parâmetro `validation_ratio` e` test_ratio`. Os valores padrão de `validation_ratio` e` test_ratio` são `0.1` e` 0.1`.\n",
        "2. Faça download de um [Vetor de recurso de imagem] (https://www.tensorflow.org/hub/common_signatures/images#image_feature_vector) como o modelo básico do TensorFlow Hub. O modelo pré-treinado padrão é EfficientNet-Lite0.\n",
        "3. Adicione um classificador head com Dropout Layer com `dropout_rate` entre a camada head e o modelo pré-treinado. O `dropout_rate` padrão é o valor` dropout_rate` padrão de [make_image_classifier_lib] (https://github.com/tensorflow/hub/blob/master/tensorflow_hub/tools/make_image_classifier/make_image_classifier_lib.py#L55) por TensorFlow Hub.\n",
        "4. Pré-processe os dados de entrada brutos. Atualmente, as etapas de pré-processamento incluem normalizar o valor de cada pixel da imagem para modelar a escala de entrada e redimensioná-la para modelar o tamanho de entrada. EfficientNet-Lite0 tem a escala de entrada `[0, 1]` e o tamanho da imagem de entrada `[224, 224, 3]`.\n",
        "5. Alimente os dados no modelo do classificador. Por padrão, os parâmetros de treinamento, como épocas de treinamento, tamanho do lote, taxa de aprendizagem, momento são os valores padrão de [make_image_classifier_lib] (https://github.com/tensorflow/hub/blob/master/tensorflow_hub/tools/make_image_classifier/make_image_classifier_lib .py # L55) do TensorFlow Hub. Apenas o cabeçote do classificador é treinado.\n",
        "\n",
        "\n",
        "Nesta seção, descrevemos vários tópicos avançados, incluindo a mudança para um modelo de classificação de imagem diferente, alterando os hiperparâmetros de treinamento etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu3n4asHaeOp"
      },
      "source": [
        "#Quantização pós-treinamento no modelo TensorFLow Lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wfFjqswarhc"
      },
      "source": [
        "[Quantização pós-treinamento] (https://www.tensorflow.org/lite/performance/post_training_quantization) é uma técnica de conversão que pode reduzir o tamanho do modelo e a latência de inferência, ao mesmo tempo que melhora a latência do acelerador de hardware e CPU, com pouca degradação no modelo precisão. Portanto, é amplamente utilizado para otimizar o modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MslaTGbjalAw"
      },
      "source": [
        "O Model Maker oferece suporte a várias opções de quantização pós-treinamento. Vamos tomar a quantização inteira inteira como uma instância. Primeiro, defina a configuração de quantização para forçar a quantização inteira completa para todas as operações, incluindo a entrada e a saída. O tipo de entrada e o tipo de saída são `uint8` por padrão. Você também pode alterá-los para outros tipos como `int8` definindo` inference_input_type` e `inference_output_type` na configuração."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opv-2RcNaLxc"
      },
      "source": [
        "#config = configs.QuantizationConfig.create_full_integer_quantization(representative_data=test_data, is_integer_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAINionebCWQ"
      },
      "source": [
        "Em seguida, exportamos o modelo TensorFlow Lite com essa configuração."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8j8gJmzbC0R"
      },
      "source": [
        "#model.export(export_dir='.', tflite_filename='model_quant.tflite', quantization_config=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpdcqPKWbONL"
      },
      "source": [
        "No Colab, você pode baixar o modelo chamado model_quant.tflite da barra lateral esquerda, igual à parte de upload mencionada acima."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdrMutyQbXrN"
      },
      "source": [
        "#Mudar o modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR1BFK_2b2vV"
      },
      "source": [
        "Mude para o modelo compatível com esta biblioteca.\n",
        "Esta biblioteca é compatível com os modelos EfficientNet-Lite, MobileNetV2, ResNet50 agora. EfficientNet-Lite é uma família de modelos de classificação de imagem que podem alcançar precisão de última geração e adequados para dispositivos Edge. O modelo padrão é EfficientNet-Lite0.\n",
        "\n",
        "Poderíamos mudar o modelo para MobileNetV2 apenas definindo o parâmetro model_spec para mobilenet_v2_spec no método de criação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROBxLbkzbQic"
      },
      "source": [
        "#model = image_classifier.create(train_data, model_spec=model_spec.mobilenet_v2_spec, validation_data=validation_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_PXadpccJqB"
      },
      "source": [
        "Avalie o modelo MobileNetV2 recém-treinado para ver a precisão e a perda nos dados de teste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg3DT-LGcN8x"
      },
      "source": [
        "#loss, accuracy = model.evaluate(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JXBJBdTcZIl"
      },
      "source": [
        "#Mudança para o modelo no TensorFlow Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKg4dPk8cl2E"
      },
      "source": [
        "Além disso, também poderíamos mudar para outros novos modelos que inserem uma imagem e geram um vetor de recursos com o formato TensorFlow Hub.\n",
        "\n",
        "Como modelo [Inception V3] (https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1) como um exemplo, poderíamos definir `inception_v3_spec` que é um objeto de` ImageModelSpec` e contém a especificação do Inception Modelo V3.\n",
        "\n",
        "Precisamos especificar o nome do modelo `name`, o url do modelo TensorFlow Hub` uri`. Enquanto isso, o valor padrão de `input_image_shape` é` [224, 224] `. Precisamos alterá-lo para `[299, 299]` para o modelo Inception V3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNcuE07ycc47"
      },
      "source": [
        "#inception_v3_spec = model_spec.ImageModelSpec(\n",
        "    #uri='https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1')\n",
        "#inception_v3_spec.input_image_shape = [299, 299]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvOVLDf_dPKa"
      },
      "source": [
        "Então, definindo o parâmetro `model_spec` para` inception_v3_spec` no método `create`, poderíamos treinar novamente o modelo Inception V3.\n",
        "\n",
        "As etapas restantes são exatamente as mesmas e poderíamos obter um modelo personalizado do InceptionV3 TensorFlow Lite no final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HdnxngUdu2N"
      },
      "source": [
        "#Mude seu próprio modelo personalizado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M_PcvmteKVn"
      },
      "source": [
        "Se quisermos usar o modelo personalizado que não está no TensorFlow Hub, devemos criar e exportar [ModelSpec] (https://www.tensorflow.org/hub/api_docs/python/hub/ModuleSpec) no TensorFlow Hub.\n",
        "\n",
        "Então comece a definir o objeto `ImageModelSpec` como o processo acima."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p5oOxOjeU-j"
      },
      "source": [
        "#Alterar os hiperparâmetros de treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhKRe_ZDedqU"
      },
      "source": [
        "Também podemos alterar os hiperparâmetros de treinamento como `epochs`,` dropout_rate` e `batch_size` que podem afetar a precisão do modelo. Os parâmetros do modelo que você pode ajustar são:\n",
        "\n",
        "\n",
        "* `epochs`: mais épocas podem alcançar melhor precisão até que converta, mas treinar por muitas épocas pode levar a overfitting.\n",
        "* `dropout_rate`: A taxa de abandono, evite overfitting. Nenhum por padrão.\n",
        "* `batch_size`: número de amostras para usar em uma etapa de treinamento. Nenhum por padrão.\n",
        "* `validation_data`: dados de validação. Se nenhum, ignora o processo de validação. Nenhum por padrão.\n",
        "* `train_whole_model`: se verdadeiro, o módulo Hub é treinado junto com a camada de classificação no topo. Caso contrário, treine apenas a camada de classificação superior. Nenhum por padrão.\n",
        "* `learning_rate`: Taxa de aprendizagem básica. Nenhum por padrão.\n",
        "* `momentum`: um float do Python encaminhado para o otimizador. Só usado quando\n",
        "      `use_hub_library` é True. Nenhum por padrão.\n",
        "*   `shuffle`: Booleano, se os dados devem ser misturados. Falso por padrão.\n",
        "*   `use_augmentation`: Booleano, usa aumento de dados para pré-processamento. Falso por padrão.\n",
        "* `use_hub_library`: Booleano, use` make_image_classifier_lib` do hub tensorflow para treinar novamente o modelo. Este pipeline de treinamento pode alcançar melhor desempenho para conjuntos de dados complicados com muitas categorias. Verdadeiro por padrão.\n",
        "* `warmup_steps`: Número de etapas de aquecimento para a programação de aquecimento na taxa de aprendizagem. Se Nenhum, o warmup_steps padrão é usado, que é o total de etapas de treinamento em duas épocas. Usado apenas quando `use_hub_library` é False. Nenhum por padrão.\n",
        "* `model_dir`: Opcional, a localização dos arquivos de checkpoint do modelo. Usado apenas quando `use_hub_library` é False. Nenhum por padrão.\n",
        "\n",
        "Parâmetros que são Nenhum por padrão, como `epochs`, obterão os parâmetros padrão concretos em [make_image_classifier_lib] (https://github.com/tensorflow/hub/blob/02ab9b7d3455e99e97abecf43c5d598a5528e20c/tensorflow_hub/tools/make_image_classificador de imagem54) Biblioteca do TensorFlow Hub ou [train_image_classifier_lib] (https://github.com/tensorflow/examples/blob/f0260433d133fd3cea4a920d1e53ecda07163aee/tensorflow_examples/lite/model_maker/core/task/train_imagelib.class).\n",
        "\n",
        "Por exemplo, podemos treinar com mais épocas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNvgI9q4dP96"
      },
      "source": [
        "#model = image_classifier.create(train_data, validation_data=validation_data, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM1GNGLjfj9a"
      },
      "source": [
        "Avalie o modelo recém-treinado com 10 épocas de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPhfAMRbfkdk"
      },
      "source": [
        "#loss, accuracy = model.evaluate(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
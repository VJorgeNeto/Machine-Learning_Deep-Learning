{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhQa9gFZ9iIB1N44HmpMeM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VJorgeNeto/Machine-Learning_Deep-Learning/blob/main/Simple_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim"
      ],
      "metadata": {
        "id": "5jtxv3RTNI8Y"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor()# Definindo a conversão de imagens para Tensor\n",
        "\n",
        "trainset = datasets.MNIST('./MNIST_data', download=True, train=True, transform=transform) # Carrega a parte de treino do Dataset\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) # Cria um buffer para pegar os dados por partes\n",
        "\n",
        "valset = datasets.MNIST('./MNIST_data', download=True, train=False, transform=transform) # Carrega a parte de validação do Dataset\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True) # Cria um buffer para pegar os dados por partes"
      ],
      "metadata": {
        "id": "WdS_veD6OTWq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificaremos agora estamos conseguindo ver as imagens do Dataset\n",
        "\n",
        "dataiter = iter(trainloader)\n",
        "imagens, etiquetas = dataiter.next()\n",
        "plt.imshow(imagens[0].numpy().squeeze(), cmap='gray_r')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "6WPoehUGQmEd",
        "outputId": "1672212d-c8ad-4ba4-edac-6332947947b9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7ad46024d0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(imagens[0].shape) # Para verificar as dimensões do Tensor de cada imagem\n",
        "print(etiquetas[0].shape) # Para verificar as dimensões do Tensor de cada etiqueta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAzSeHUORpq1",
        "outputId": "f4205c24-7604-4dd3-c8be-9620e336833f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Modelo(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Modelo, self).__init__()\n",
        "    self.linear1 = nn.Linear(28*28, 128) # Camada de entrada, 784 neurônios que se ligam a 128\n",
        "    self.linear2 = nn.Linear(128, 64) # Camada interna 1, 128 neurônios que se ligam a 64\n",
        "    self.linear3 = nn.Linear(64, 10) # Camada interna 2, 64 neurônios que se ligam a 10\n",
        "    # Para a camada de saída não é necessário definir nada pois só precisamos pegar o output da camada interna 2\n",
        "\n",
        "  def forward(self, X):\n",
        "    X = F.relu(self.linear1(X)) # Função da ativação da camada de entrada para a camada interna 1\n",
        "    X = F.relu(self.linear2(X)) # Função de ativação da camada interna 1 para a camada interna 2\n",
        "    X  = self.linear3(X) # Função de ativação da camada interna 2 para a camada de saída, nesse caso f(x) = x\n",
        "    return F.log_softmax(X, dim=1) # Dados utilizados para calcular a perda"
      ],
      "metadata": {
        "id": "rOoNt1uQTLkh"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from prompt_toolkit import output\n",
        "def treino(modelo, trainloader, device):\n",
        "  otimizador = optim.SGD(modelo.parameters(), lr=0.01, momentum=0.5) # Define a política de atualização dos pesos e da bias\n",
        "  inicio = time() # Timer para sabermos quanto tempo levou o treino\n",
        "\n",
        "  criterio = nn.NLLLoss() # Definindo o critério para calcular a perda\n",
        "  EPOCHS = 30 # Número de epochs que o algoritmo rodará\n",
        "  modelo.train() # Ativando o modo de treinamento do modelo\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    perda_acumulada = 0 # Inicialização da perda acumulada da epoch em questão\n",
        "\n",
        "    for imagens, etiquetas in trainloader:\n",
        "\n",
        "      imagens = imagens.view(imagens.shape[0], -1) # Convertendo as imagens para \"vetores\" de 28*28 casas para ficarem compatíveis\n",
        "      otimizador.zero_grad() # Zerando os gradientes por conta do ciclo anterior\n",
        "\n",
        "      output = modelo(imagens.to(device)) # Colocando os dados no modelo\n",
        "      perda_instantanea = criterio(output, etiquetas.to(device)) # Calculando a perda da epoch em questão\n",
        "\n",
        "      perda_instantanea.backward() # Back propagation a partir da perda\n",
        "\n",
        "      otimizador.step() # Atualizando os pesos e a bias\n",
        "\n",
        "      perda_acumulada += perda_instantanea.item() # Atualização da perda acumulada\n",
        "\n",
        "    else:\n",
        "      print(\"Epoch {} - Perda resultante: {}\".format(epoch+1, perda_acumulada/len(trainloader)))\n",
        "  print(\"\\nTempo de treino (em minutos) =\",(time()-inicio)/60)"
      ],
      "metadata": {
        "id": "dKWRjnwFWtLq"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validacao(modelo, valloader, device):\n",
        "  conta_corretas, conta_todas = 0, 0\n",
        "  for imagens,etiquetas in valloader:\n",
        "    for i in range(len(etiquetas)):\n",
        "      img = imagens[i].view(1, 784)\n",
        "      # Desativar o autograd para acelerar a validação. Grafos computacionais dinâmicos tem um custo alto de processamento\n",
        "      with torch.no_grad():\n",
        "        logps = modelo(img.to(device)) # output do modelo em escala logaritmica\n",
        "\n",
        "      ps = torch.exp(logps) # Converte output para escala normal(lembrando que é um tensor)\n",
        "      probab = list(ps.cpu().numpy()[0])\n",
        "      etiqueta_pred = probab.index(max(probab)) # Converte o tensor em um número, no caso, o número que o modelo previu como coreeto\n",
        "      etiqueta_certa = etiquetas.numpy()[i]\n",
        "      if (etiqueta_certa == etiqueta_pred): # Compara previsão com um valor correto\n",
        "        conta_corretas += 1\n",
        "      conta_todas += 1\n",
        "  \n",
        "  print(\"Total de imagens testadas =\", conta_todas)\n",
        "  print(\"\\nPrecisão do modelo = {}%\".format(conta_corretas*100/conta_todas))"
      ],
      "metadata": {
        "id": "rdYQON0wbYQ5"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = Modelo()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "modelo.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roNVX-wWesN8",
        "outputId": "15636e1d-7738-4321-88ef-d74371649921"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Modelo(\n",
              "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ]
}